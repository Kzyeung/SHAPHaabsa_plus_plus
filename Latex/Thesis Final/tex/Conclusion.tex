\chapter{Conclusion}
    \label{chap:conclusion}
SHAP has proven to be a feasible way to better understand model behaviour. SHAP is able to infer local and global interpretations, offering the user insights into why a model did, does, and will do something. In contrast to many other interpretation methods, SHAP has the added benefit of not having to exchange accuracy for comprehension. The results in the form of SHAP values are easily interpretable, as they are linearly related to each other.

The complexity of the LCR-Rot-hop++ model that functions as backup method to the HAABSA++ method does lead to intricacies. Because the LCR-Rot-Hop++ model utilizes contextually aware BERT embeddings, features are never completely separable from each other. Thus, SHAP may sometimes be misleading, in the sense that a word can contain information about other words in the form of contextual information. 

SHAP model 1 and SHAP model 2 were designed to evaluate two different methods to better capture the behaviour of the LCR-Rot-hop++ model. We conclude that SHAP model 1 is more likely to capture the contribution of specific contexts, since words maintain their information about the original context, even after being subsetted by SHAP. SHAP model 2 partially refutes this, by generating new BERT embeddings for each subset of the original sentence. As a result, SHAP model 2 is better at capturing the contribution of specific words, which can lead to a change in context. Thus, words with semantic meaning are more likely to assigned a high contribution than function words that depend on their context. Nonetheless, function words still add significant obscurities to the results, for both SHAP model 1 and SHAP model 2. Future research could consider (partially) removing function words before feeding them to the SHAP model, to avoid dilution of the meaning of other words. 

Once SHAP values are calculated, aggregating and averaging these SHAP values offer an accessible way to obtain global insights about a model's behaviour. Although SHAP global model is very computationally intensive to train, it does show potential towards gaining new insights. One could potentially derive specific characteristics of habits and word usage for certain sentiment classifications by analyzing the average SHAP values of each word. For instance, an exclamation mark \textit{`!'} does not add a polarity to the sentence in the form of meaning, but it could point towards a tendency towards certain write styles when excited or happy. However, the possible insights about model behaviour derived from this are more difficult to determine with the current output and should be further researched in the future. 

Although SHAP offers a lot of potential to gain a better understanding of model behaviour, it is not without its disadvantages. One of the main problems with SHAP is the required computational power, as a power set of a sentence contains a high number of subsets that need to be predicted. Additionally, although SHAP model 2 proves to be more effective at defining contributions of individual words, the fact that new embeddings have to be created for every single subset adds another layer that is computationally intensive. The earlier suggestion to potentially remove function words could compensate by reducing the amount of subsets, but it could also be worth exploring methods that only estimate Shapley values instead of precise calculations \cite{Aas, Ancona, Chen}. Altogether, SHAP shows that is has potential to grow on both possible insights it can reveal, as well as its efficiency.