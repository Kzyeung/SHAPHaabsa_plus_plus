\chapter{Introduction}
	\label{chap:introduction}
This chapter depicts the problem that our research encompasses, as well as the relevance and the objective of the research. First, an explanation of the problem framework will be given in Section \ref{sect:intro_problem}, while a more specific context of the research will be given in Section \ref{sect:intro_objectives}, therefore defining the the research objective. Chapter \ref{chap:literature_review} reviews prior academic literature relating to our topic and research. Next, Chapter \ref{chap:data} will describe the data and characteristics of the data used in our research, as well as the processing and cleaning methods used. Chapter \ref{chap:methodology} continues to explain the methodology of the mechanisms and models used in our research. Finally, Chapter \ref{chap:results} will present the results of our research, and Chapter \ref{chap:conclusion} will give an outline of our most important gained insights.

\section{Problem Statement}
    \label{sect:intro_problem}
The mass integration of the Social Web into consumer markets has drastically altered interactions between businesses and consumers; specifically reviews have had a significant impact as a form of customer value co-creation \cite{Shin}. Driven by innovations in digital technology, customer behavior has substantially changed as a response to the digital revolution \cite{Verhoef2}. As a result, customers are more connected, informed, empowered, and active \cite{Lamberton,Verhoef1}. Consequently, academic research into reviews has seen a huge increase in coverage, with sentiment analysis being one of the main fields of interests within computer and data science \cite{Liu}. Sentiment analysis makes it possible to automate the evaluation of review sentiments, enabling managers and researchers to derive insights on a large scale, which have had huge implications for various sectors. This is necessary, as the number of reviews is immense and increases with each passing day, making it increasingly more resource-intensive to analyze reviews manually.

The simplest application of sentiment analysis is the sentiment classification of sentences, paragraphs, or documents. This process, however, is unable to capture all the complexities of human language, as the meaning of language is more dynamic and influenced by factors such as context and presumptions. One way of dealing with context is Aspect Based Sentiment Analysis (ABSA); here, the relevant entity or related aspect is identified and then classified according to sentiment \cite{Schouten2}. Several varieties of ABSA models have emerged over the years, resulting in impressive performances.

As high performing models often combine multiple techniques to increase accuracy, the transparency of such models decreases, resulting in black box models. The black box nature of a model is problematic in various ways, as it reduces the comprehension of why a model did, does, and will do something. Even if correct, the model prediction could be based on the wrong reasoning or predictors, potentially leading to decisions based on compliance, rather than truthful insights. Regardless, a deep comprehension of a model is beneficial, as it lends credence and leads to increased insights. To achieve this, it is important to not only achieve interpretability, but also explainability. In short, interpretability refers to the ability to predict model behavior, while explainability refers to understanding model behavior. To obtain explainability of a model, one needs to be able to summarize the reasons for behavior, with the capacity to defend its outcomes, and provide relevant responses to questions \cite{Gilpin}. Section \ref{sec:lit_interpret_explain} further elaborates on the relevance and the differences between interpretability and explainability.

\section{Research Objectives}
\label{sect:intro_objectives}
This research aims to increase explainability of ABSA models -- particularly to aid managers and researchers in comprehending model behavior -- with the goal to create trust and insights about how decisions are made \cite{Gilpin}. Aiding this purpose, is the state-of-the-art ABSA model that have been developed for the SemEval 2015 and 2016 tasks and associated data, namely the HAABSA++ model as proposed by \cite{Trusca}. The model is an extension on the HAABSA model designed by \cite{Wallaart}, where an ontology-driven approach proposed by \cite{Schouten} and a deep learning model named the LCR-rot-hop model based on \cite{Zheng}, are combined into a hybrid model.

The hybrid model uses an ontology-driven approach to predict sentiment, while the deep learning acts as back-up for when the ontology-driven approach proves inconclusive. The HAABSA++ model proposed by \cite{Trusca} extends the LCR-rot-hop model by adding a hierarchical attention layer to the deep learning algorithm, improving the method flexibility. HAABSA++ also replaces the non-contextual word embeddings in HAABSA with BERT word embeddings, which is able to take into account the context around each word. HAABSA++ is a state-of-the-art ABSA model that significantly outperform the baselines for the SemEval 2015 and 2016 tasks. As the model benefits from multiple iterations of extensions, the deep learning model has increased in complexity, further decreasing the explainability of model behavior.

Since the deep learning algorithms in the HAABSA++ model has reached a highly developed stage, it is of crucial importance that tools for interpretation stay up to par to be able to understand model behavior. Therefore, this paper will focus on the following research question:

\begin{quote}
\emph{To what extent can we correctly interpret and explain the behavior of the HAABSA++ model?}
\end{quote}

To obtain model interpretability and explainability, post-hoc model-agnostic interpretation methods are used. Model-agnostic interpretation methods can be divided into global and local interpretability interpretation methods. Local interpretation methods concentrate on (detailed) explanations of single instances, while global interpretation methods describe overall model behavior. Therefore, local interpretation methods are suitable for exploring the reason behind certain unreasonable or unexpected predictions, which gives insights on why a model predicts this outcome. On the other hand, global explanation models are useful in summarizing the reasons for overall behavior, which provides insights on the whole model, making it easier to identify trends or divergence points. Thus, this work explores both local and global interpretation models to form a basis for further research into model explainability.

Various researchers have attempted to interpret the HAABSA++ model and its predecessors. This paper builds upon previous research in two ways. First, the research introduces new interpretation methods to analyze local model predictions. Second, in addition to local interpretation methods, this research builds a basis for global interpretation of the HAABSA++ model. 

To explain HAABSA++ and HAABSA* model behavior, two methods are considered. SHAP will use the concept of Shapley values to capture the marginal contribution of features on the model prediction. In addition to local explanation, SHAP has the added benefit to aggregate to a global interpretation representation, giving a clear depiction of which words contribute the most to the sentiment prediction on a global level. Third, Permutation Feature Importance (PFI) will be used as another method that calculates global feature importance. With these in mind, the following research sub-questions are formed to further aid the research:

\begin{enumerate}
    \item{Which model-agnostic interpretation methods are most accurate in predicting HAABSA++ model behavior?}
    \item{Which model-agnostic interpretation methods offer the best explainability for the HAABSA++ model?}
\end{enumerate}