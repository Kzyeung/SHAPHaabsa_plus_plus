\chapter{Literature Review}
    \label{chap:literature_review}

This chapter describes prior academic literature relevant to the topic of state-of-the-art aspect-based sentiment analysis and the interpretation thereof. Section \ref{sec:lit_HAABSA} gives an overview of the background and development of the state-of-the-art HAABSA++ model. Next, Section \ref{sec:lit_black_box} goes into existing work on the relevance of understanding complex machine learning models. Section \ref{sec:lit_interpret_explain} further expands upon the distinctions and importance of interpretability and explainability. Finally, \ref{sec:lit_model_agnostic} describes academic work that laid the groundwork for the model-agnostic methods relevant to this research.

\section{Hybrid Approach to Aspect Based Sentiment Analysis}
\label{sec:lit_HAABSA}
As previously mentioned, the state-of-the-art model of interest offers a hybrid approach to ABSA. First, an ontology-based approach is considered to predict the sentiment value, as detailed in \cite{Wallaart}. When the ontology-based approach proves inconclusive, the model makes use of a back-up deep learning model. The deep learning model, referred to as the LCR-Rot-hop model in \cite{Wallaart}, divides a sentence in three parts based on the identified target. The so-called Left-Center-Right model details the target (i.e. the target of focus in the sentence), as well as its left and right context, which together form the complete sentence. Attention weights are thereafter assigned to evaluate the most important words in the aspect and contexts \cite{Zheng}. Next, a rotary attention mechanism iterates several times to model the relation between aspect and contexts.

HAABSA++ extends the model by replacing the non-contextual GloVe word embeddings with deep contextual word embeddings with BERT \cite{Devlin}. The LCR-Rot-hop model is also updated by incorporating hierarchical attention, offering a high-level representation of the input sentence. This is done by supplementing the rotary attention with a new attention layer, as the context vectors are updated with a relevance score in each iteration of the rotary attention \cite{Trusca}.

\section{Understanding Black Box Models}
    \label{sec:lit_black_box}
Although an increase in complexity does not necessarily relate to an increase in accuracy \cite{Rudin}, complex tasks where the sole purpose lies in maximizing the accuracy often do result in complex models with a lack of transparency, also known as black box models \cite{London}. Black box models have been problematic in high-stakes decision making, as a lack of understanding can lead to undesired outcomes \cite{Rudin}. Although \cite{Rudin} states that black box models can still be used for the knowledge discovery process -- which fits the scope of the utility of the HAABSA++ model -- the mentioned problems are still relevant. A lack of understanding can lead to unjustified confidence in a model's external validity, complex decision pathways prone to human error, and a general lack of trust in the model's predictions. Hence, interpretability is often very important for a model to be usable, sometimes to the point that an increase in interpretability justifies a decrease in accuracy \cite{Ribeiro}. Model-agnostic interpretation models aid in comprehending a model's behavior, often in exchange for some of its accuracy (in case of similar performance, the interpretation model would become the main model).

As \cite{Rudin} makes clear, interpretation models still deal with various problems on top of decreased predictive power, such as insufficient detail to understand the model and incompatibility with additional information outside the given data. Thus, it is not only critical that a model can be interpreted, but it also needs to be complete, with the ability to defend its actions and provide relevant responses to questions. 

\section{Interpretability and Explainability}
    \label{sec:lit_interpret_explain}
\cite{Gilpin} argues that interpretability alone is insufficient; explainability is crucial for humans to gain trust in black box models. Although the differences between interpretability and explainability are often obscure, there are important reasons to distinguish between the two. The difference mainly lies in understanding model behavior, rather than just correctly predicting model behavior; i.e., interpretability refers to the ability to predict model outcomes, while explainability refers to understanding the relationship between output and input. Explainability ensures interpretability, but the opposite is not always the case. Consequently, this research intents to explore not only model interpretation, but also model explainability. As previously stated, understanding model behavior means that one is able to summarize and defend model behavior, as well as provide relevant responses to questions. To achieve this, global model-agnostic methods are explored on top of local model-agnostic methods. While local model-agnostic methods only focus on the vicinity of the instance one wishes to explain \cite{Ribeiro}, global model-agnostic methods aim to understand the model in its entirety, enabling the possibility to summarize model behavior. Local interpretability is more readily applicable than global interpretability, as it only needs to stay faithful to the vicinity of the instance; thus, with global interpretability, predictive power is often exchanged for a more helpful overall explanation, instead of many different explanations for every possible instance \cite{Adadi}. For this reason, both local and global model-agnostic methods are considered in this research.

\section{Model-agnostic methods}
    \label{sec:lit_model_agnostic}
Prior work has identified and proposed several methods that provide explanations for black box models. For instance, \cite{Meijer} used diagnostic classification to explain the LCR-Rot-hop deep learning model from \cite{Wallaart}. \cite{Geed} expands upon this work by using diagnostic classification to explain the LCR-Rot-hop++ model used in the HAABSA++ model. Overall, all interpretation methods around the HAABSA model and its variants explore local interpretations only, while mostly evaluating how much they correspond to the outcomes of said models.

The main method of our interest is based on Shapley values, which originate from game theory \cite{Shapley}. Shapley values calculate the marginal contribution of each feature to determine a fair payout. Various researchers have successfully adopted Shapley values in explaining machine learning models \cite{Cohen,Ghorbani,Lundberg}. Although precisely computing Shapley values is resource-intensive, many advances have been made in efficiently approximating Shapley values \cite{Aas,Ancona,Chen}. Therefore, Shapley values present a viable method to calculate the contribution of each word to the sentiment. Additionally, \cite{Lundberg} proposes SHAP; inspired by LIME, SHAP offers a united approach to explain any machine learning model's individual output. Furthermore, SHAP offers the benefit of being able to offer global interpretability, by aggregating all Shapley values, such as in \cite{Feng,Lundberg2}. Global interpretation allows the user to infer insights that can be generalized over the entire model, offering a better understanding of the flaws, strong suits, and behaviour of the model.